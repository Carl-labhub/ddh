---
title: "Entrez summary"
output: html_notebook
---

##Load libraries & clear environment
```{r}
library(tidyverse)
library(lubridate)
library(rvest)
library(janitor)
library(feather)
library(here)
library(beepr)
library(vroom)

rm(list=ls()) #clear environment

#how long?
start_time <- Sys.time()
```

#downlaod entrez gene ids from hugo
```{r}
#from https://www.genenames.org/cgi-bin/download/custom?col=gd_hgnc_id&col=gd_app_sym&col=gd_app_name&col=gd_prev_sym&col=gd_aliases&col=gd_pub_chrom_map&col=gd_pub_refseq_ids&col=md_eg_id&col=gd_locus_type&col=md_mim_id&col=md_prot_id&status=Approved&hgnc_dbtag=on&order_by=gd_app_sym_sort&format=text&submit=submit

hugo <- vroom(here::here("data", "hugo.txt"), delim = "\t", col_names = TRUE) %>% 
  clean_names()
```

#scrape data
Using: https://github.com/tidyverse/rvest
From: https://www.ncbi.nlm.nih.gov/gene/

```{r}
entrez <- tibble(
  i = numeric(), 
  entrez_symbol = character(), 
  entrez_category = character(),
  entrez_summary = character()
)

ids <- hugo %>% 
  drop_na(ncbi_gene_id_supplied_by_ncbi) %>% 
  sample_n(10) %>% 
  pull(ncbi_gene_id_supplied_by_ncbi)
  
for (i in ids) {
  message("Getting entry ", i)
  Sys.sleep(5) #add sleepy time according to https://www.ncbi.nlm.nih.gov/robots.txt
  url <- read_html(paste0("https://www.ncbi.nlm.nih.gov/gene/", i))
  
  entrez_symbol <- url %>% 
    html_nodes("dd.noline") %>% #got from inspect element
    html_text() %>% 
    str_remove("provided\\sby\\sHGNC")
  
  entrez_category <- url %>% 
    html_nodes("dt:nth-child(19)") %>% #got from inspect element
    html_text()
  
  entrez_summary <- url %>% 
    html_nodes("dd:nth-child(20)") %>% #got from inspect element
    html_text()
  
  tmp <- tibble(i, entrez_symbol, entrez_category, entrez_summary)

  entrez <- entrez %>% 
    bind_rows(tmp)
}

entrez <- entrez %>% 
  filter(entrez_category == "Summary")

#The rvest code scrapes by position in the <div>, but depenidng on which information is available, the summary position might provide other information. So I get all the summary, as well as the junk, and then filter out the junk with the last line of code

beep(sound = 8) #because mario is awesome
```

#how long to scrape?
```{r}
end_time <- Sys.time()
time_taken <- round(as.duration(start_time %--% end_time)/dminutes(1), digits = 1)
print(time_taken)
```
Approximate time to run was `r time_taken` minutes.

#save
```{r}
write_feather(entrez, path = here::here("entrez.feather"))
write_csv(entrez, path = here::here("entrez.csv"))
```

#print Session information for provenance and reproducibility
```{r session_info}
utils:::print.sessionInfo(sessionInfo()[-8]) 
#You can remove an item from sessionInfo(), which is a list with a class attribute, by printing the resulting object omitting one of the list items (omitted list of packages installed, but not loaded)
```
